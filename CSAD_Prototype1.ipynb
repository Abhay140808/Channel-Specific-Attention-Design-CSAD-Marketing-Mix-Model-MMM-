{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Idwgr4IUvDEk",
        "outputId": "e1e6e6fb-cb42-499f-b167-0fe9aa0aa1dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exists: False\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "print(\"Exists:\", os.path.exists('/content/drive/MyDrive/csad_project/data/multi_region_mmm.csv'))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sxuFzYBvWKB",
        "outputId": "4c7f78f6-9f7f-4c11-fd03-0d47b65adeed"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# ðŸ“¦ Data Preparation\n",
        "# ===============================\n",
        "\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/csad_project/data/multi_region_mmm.csv')\n",
        "df['DATE_DAY'] = pd.to_datetime(df['DATE_DAY'])\n",
        "df['week'] = df['DATE_DAY'].dt.isocalendar().week\n",
        "df['year'] = df['DATE_DAY'].dt.year\n",
        "\n",
        "\n",
        "agg_df = df.groupby(['ORGANISATION_ID', 'TERRITORY_NAME', 'week', 'year']).agg({\n",
        "    'GOOGLE_PAID_SEARCH_SPEND': 'sum',\n",
        "    'META_FACEBOOK_SPEND': 'sum',\n",
        "    'META_INSTAGRAM_SPEND': 'sum',\n",
        "    'GOOGLE_VIDEO_SPEND': 'sum',\n",
        "    'EMAIL_CLICKS': 'sum',\n",
        "    'REFERRAL_CLICKS': 'sum',\n",
        "    'GOOGLE_DISPLAY_SPEND': 'sum',\n",
        "    'BRANDED_SEARCH_CLICKS': 'sum',\n",
        "    'ALL_PURCHASES': 'sum'\n",
        "}).reset_index()\n",
        "\n",
        "\n",
        "agg_df = agg_df.rename(columns={\n",
        "    'ORGANISATION_ID': 'brand_id',\n",
        "    'TERRITORY_NAME': 'region',\n",
        "    'GOOGLE_PAID_SEARCH_SPEND': 'search',\n",
        "    'META_FACEBOOK_SPEND': 'facebook',\n",
        "    'META_INSTAGRAM_SPEND': 'instagram',\n",
        "    'GOOGLE_VIDEO_SPEND': 'video',\n",
        "    'EMAIL_CLICKS': 'email',\n",
        "    'REFERRAL_CLICKS': 'affiliate',\n",
        "    'GOOGLE_DISPLAY_SPEND': 'display',\n",
        "    'BRANDED_SEARCH_CLICKS': 'promotion',\n",
        "    'ALL_PURCHASES': 'sales'\n",
        "})\n",
        "\n",
        "\n",
        "agg_df['week_idx'] = pd.factorize(agg_df['year'].astype(str) + '-' + agg_df['week'].astype(str))[0]\n",
        "agg_df['sales'] = (agg_df['sales'] - agg_df['sales'].mean()) / (agg_df['sales'].std() + 1e-6)\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# ðŸ§± Dataset\n",
        "# ===============================\n",
        "\n",
        "\n",
        "class CSADSequenceDataset(Dataset):\n",
        "    def __init__(self, df, window_size=32):\n",
        "        self.window_size = window_size\n",
        "        self.media_cols = ['search', 'facebook', 'instagram', 'video', 'email', 'affiliate', 'display', 'promotion']\n",
        "        self.brand2idx = {b: i for i, b in enumerate(df['brand_id'].unique())}\n",
        "        self.region2idx = {r: i for i, r in enumerate(df['region'].unique())}\n",
        "        df['brand_idx'] = df['brand_id'].map(self.brand2idx)\n",
        "        df['region_idx'] = df['region'].map(self.region2idx)\n",
        "\n",
        "\n",
        "        self.samples = []\n",
        "        for (brand, region), group in df.groupby(['brand_id', 'region']):\n",
        "            group = group.sort_values(by='week_idx')\n",
        "            if len(group) < window_size + 1:\n",
        "                continue\n",
        "            for i in range(len(group) - window_size):\n",
        "                window = group.iloc[i:i+window_size]\n",
        "                target = group.iloc[i+window_size]['sales']\n",
        "                self.samples.append({\n",
        "                    'media_seq': window[self.media_cols].values.astype(np.float32),\n",
        "                    'week_idx_seq': window['week_idx'].values.astype(np.int64),\n",
        "                    'brand_idx': window['brand_idx'].iloc[0],\n",
        "                    'region_idx': window['region_idx'].iloc[0],\n",
        "                    'target_sales': target\n",
        "                })\n",
        "\n",
        "\n",
        "    def __len__(self): return len(self.samples)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        s = self.samples[idx]\n",
        "        return {\n",
        "            'media_seq': torch.tensor(s['media_seq']),\n",
        "            'week_idx_seq': torch.tensor(s['week_idx_seq']),\n",
        "            'brand_idx': torch.tensor(s['brand_idx']),\n",
        "            'region_idx': torch.tensor(s['region_idx']),\n",
        "            'target_sales': torch.tensor(s['target_sales'], dtype=torch.float32)\n",
        "        }\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# ðŸ§  Model\n",
        "# ===============================\n",
        "\n",
        "\n",
        "class PositionalEncoder(nn.Module):\n",
        "    def __init__(self, max_weeks, embed_dim):\n",
        "        super().__init__()\n",
        "        self.week_embedding = nn.Embedding(max_weeks, embed_dim)\n",
        "\n",
        "\n",
        "    def forward(self, week_idx):\n",
        "        return self.week_embedding(week_idx)\n",
        "\n",
        "\n",
        "class VariableSelectionGate(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.Linear(input_dim, input_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(input_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        weight = self.gate(x)\n",
        "        return weight * x\n",
        "\n",
        "\n",
        "class ChannelEncoder(nn.Module):\n",
        "    def __init__(self, week_embed_dim=8, hidden_dim=32):\n",
        "        super().__init__()\n",
        "        self.input_dim = 1 + week_embed_dim\n",
        "        self.proj = nn.Linear(self.input_dim, hidden_dim)\n",
        "        self.attn_score = nn.Linear(hidden_dim, 1)\n",
        "        self.norm = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "\n",
        "    def forward(self, spend_seq, week_embed):\n",
        "        x = torch.cat([spend_seq, week_embed], dim=-1)\n",
        "        x_proj = self.proj(x)\n",
        "        scores = self.attn_score(x_proj).squeeze(-1)\n",
        "        weights = torch.softmax(scores, dim=1).unsqueeze(-1)\n",
        "        x_weighted = (x_proj * weights).sum(dim=1)\n",
        "        return self.norm(x_weighted)\n",
        "\n",
        "\n",
        "class GraphGuidtedMultiheadAttenion(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, num_channels):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
        "        self.graph_adj = nn.Parameter(torch.ones(num_channels, num_channels))\n",
        "\n",
        "\n",
        "    def forward(self, q, k, v):\n",
        "        attn_out, attn_weights = self.attn(q, k, v)\n",
        "        masked_weights = attn_weights * self.graph_adj.unsqueeze(0)\n",
        "        output = torch.bmm(masked_weights, v)\n",
        "        return output, masked_weights\n",
        "\n",
        "\n",
        "class CSADModel(nn.Module):\n",
        "    def __init__(self, num_channels=8, week_embed_dim=8, hidden_dim=64, context_embed_dim=8,\n",
        "                 brand_vocab_size=100, region_vocab_size=20, max_weeks=300):\n",
        "        super().__init__()\n",
        "        self.pos_encoder = PositionalEncoder(max_weeks, week_embed_dim)\n",
        "        self.gates = nn.ModuleList([VariableSelectionGate(1) for _ in range(num_channels)])\n",
        "        self.channel_encoders = nn.ModuleList([ChannelEncoder(week_embed_dim, hidden_dim) for _ in range(num_channels)])\n",
        "        self.cross_attn = GraphGuidtedMultiheadAttenion(embed_dim=hidden_dim, num_heads=1, num_channels=num_channels)\n",
        "        self.cross_norm = nn.LayerNorm(hidden_dim * num_channels)\n",
        "        self.brand_emb = nn.Embedding(brand_vocab_size, context_embed_dim)\n",
        "        self.region_emb = nn.Embedding(region_vocab_size, context_embed_dim)\n",
        "        self.brand_proj = nn.Linear(context_embed_dim, hidden_dim)\n",
        "        self.region_proj = nn.Linear(context_embed_dim, hidden_dim)\n",
        "        self.context_gate = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, 1), nn.Sigmoid()\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * num_channels + hidden_dim, 256), nn.ReLU(), nn.Dropout(0.3),\n",
        "            nn.Linear(256, 128), nn.ReLU(), nn.Dropout(0.2),\n",
        "            nn.Linear(128, 64), nn.ReLU(), nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, media_seq, week_idx_seq, brand_idx, region_idx):\n",
        "        B, T, C = media_seq.shape\n",
        "        week_embed = self.pos_encoder(week_idx_seq)\n",
        "        reps = []\n",
        "        for i in range(C):\n",
        "            x = media_seq[:, :, i].unsqueeze(-1)\n",
        "            x = self.gates[i](x)\n",
        "            rep = self.channel_encoders[i](x, week_embed)\n",
        "            reps.append(rep)\n",
        "        x_stack = torch.stack(reps, dim=1)\n",
        "        attn_out, _ = self.cross_attn(x_stack, x_stack, x_stack)\n",
        "        x_flat = self.cross_norm(attn_out.reshape(B, -1))\n",
        "        b_proj = self.brand_proj(self.brand_emb(brand_idx))\n",
        "        r_proj = self.region_proj(self.region_emb(region_idx))\n",
        "        gate = self.context_gate(torch.cat([b_proj, r_proj], dim=-1))\n",
        "        context = gate * b_proj + (1 - gate) * r_proj\n",
        "        return self.fc(torch.cat([x_flat, context], dim=-1)).squeeze(-1)\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# ðŸ§ª Train/Test Utilities\n",
        "# ===============================\n",
        "\n",
        "\n",
        "def build_loaders(df, window_size=32, batch_size=32):\n",
        "    dataset = CSADSequenceDataset(df, window_size)\n",
        "    group_to_indices = {}\n",
        "    for idx, s in enumerate(dataset.samples):\n",
        "        k = (int(s['brand_idx']), int(s['region_idx']))\n",
        "        group_to_indices.setdefault(k, []).append(idx)\n",
        "    all_groups = list(group_to_indices.keys())\n",
        "    gss = GroupShuffleSplit(n_splits=1, train_size=0.8, random_state=42)\n",
        "    train_g, val_g = next(gss.split(np.arange(len(all_groups)), groups=np.arange(len(all_groups))))\n",
        "    train_idx = [idx for g in train_g for idx in group_to_indices[all_groups[g]]]\n",
        "    val_idx = [idx for g in val_g for idx in group_to_indices[all_groups[g]]]\n",
        "    return dataset, DataLoader(Subset(dataset, train_idx), batch_size, shuffle=True), DataLoader(Subset(dataset, val_idx), batch_size)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def mc_dropout_predict(model, batch, T=30):\n",
        "    model.eval()\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Dropout): m.train()\n",
        "    preds = [model(batch['media_seq'], batch['week_idx_seq'], batch['brand_idx'], batch['region_idx']).cpu().numpy() for _ in range(T)]\n",
        "    return np.stack(preds).mean(0), np.stack(preds).std(0)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def counterfactual_predict(model, factual_batch, intervention_fn, T=30):\n",
        "    counter_batch = {k: v.clone() for k, v in factual_batch.items()}\n",
        "    counter_batch = intervention_fn(counter_batch)\n",
        "    return mc_dropout_predict(model, counter_batch, T)\n",
        "\n",
        "\n",
        "def train_one_epoch(model, loader, loss_fn, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        pred = model(batch['media_seq'], batch['week_idx_seq'], batch['brand_idx'], batch['region_idx'])\n",
        "        loss = loss_fn(pred, batch['target_sales'])\n",
        "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, loss_fn, device, use_mc=False):\n",
        "    model.eval(); y_true, y_pred, stds = [], [], []\n",
        "    for batch in loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        y_true.extend(batch['target_sales'].cpu().numpy())\n",
        "        if use_mc:\n",
        "            m, s = mc_dropout_predict(model, batch); y_pred.extend(m); stds.extend(s)\n",
        "        else:\n",
        "            y_pred.extend(model(batch['media_seq'], batch['week_idx_seq'], batch['brand_idx'], batch['region_idx']).cpu().numpy())\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    return mse, np.sqrt(mse), r2_score(y_true, y_pred), stds if use_mc else None\n",
        "\n",
        "\n",
        "def plot_losses(train, val):\n",
        "    plt.plot(train, label=\"Train\"); plt.plot(val, label=\"Val\")\n",
        "    plt.legend(); plt.title(\"Loss\"); plt.grid(); plt.show()\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# ðŸš€ Training Loop\n",
        "# ===============================\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "brand_vocab_size = agg_df['brand_id'].nunique()\n",
        "region_vocab_size = agg_df['region'].nunique()\n",
        "\n",
        "\n",
        "model = CSADModel(brand_vocab_size=brand_vocab_size, region_vocab_size=region_vocab_size).to(device)\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "\n",
        "\n",
        "seq_dataset, train_loader, val_loader = build_loaders(agg_df)\n",
        "\n",
        "\n",
        "train_losses, val_losses = [], []\n",
        "best_val_rmse = float('inf')\n",
        "early_stop_counter = 0\n",
        "patience = 5\n",
        "\n",
        "\n",
        "for epoch in range(25):\n",
        "    train_loss = train_one_epoch(model, train_loader, loss_fn, optimizer, device)\n",
        "    mse, rmse, r2, _ = evaluate(model, val_loader, loss_fn, device, use_mc=True)\n",
        "    train_losses.append(train_loss); val_losses.append(mse)\n",
        "    print(f\"Epoch {epoch+1:03d} | Train Loss: {train_loss:.4f} | Val RMSE: {rmse:.4f} | RÂ²: {r2:.4f}\")\n",
        "    if rmse < best_val_rmse:\n",
        "        best_val_rmse = rmse\n",
        "        best_model_state = deepcopy(model.state_dict())\n",
        "        early_stop_counter = 0\n",
        "        torch.save(model.state_dict(), \"csad_best_model.pt\")\n",
        "        print(\"âœ… Saved best model (RMSE improved)\")\n",
        "    else:\n",
        "        early_stop_counter += 1\n",
        "        if early_stop_counter >= patience:\n",
        "            print(\"ðŸ›‘ Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "\n",
        "plot_losses(train_losses, val_losses)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "9rKXn2cvvwCt",
        "outputId": "49d89b2f-c849-43a9-f47a-afa8c9da40bd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | Train Loss: 1.5858 | Val RMSE: 0.2083 | RÂ²: -0.1300\n",
            "âœ… Saved best model (RMSE improved)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4266955279.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m     \u001b[0mmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4266955279.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loader, loss_fn, optimizer, device)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'media_seq'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'week_idx_seq'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'brand_idx'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'region_idx'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target_sales'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    499\u001b[0m             \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m             \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Optimizer.step#{self.__class__.__name__}.step\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m                 \u001b[0;31m# call optimizer step pre hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m                 for pre_hook in chain(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/profiler.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDisableTorchFunctionSubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_function_exit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RecordFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    794\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_function_exit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1055\u001b[0m     \u001b[0;31m# that are named \"self\". This way, all the aten ops can be called by kwargs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_P\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_P\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_T\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1057\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0m_must_dispatch_in_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1058\u001b[0m             \u001b[0;31m# When any inputs are FakeScriptObject, we need to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0;31m# skip c++ dispatcher and dispatch in python through _get_dispatch of python_dispatcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m_must_dispatch_in_python\u001b[0;34m(args, kwargs)\u001b[0m\n\u001b[1;32m   1115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_must_dispatch_in_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m     return pytree.tree_any(\n\u001b[0m\u001b[1;32m   1118\u001b[0m         lambda obj: isinstance(\n\u001b[1;32m   1119\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_library\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfake_class_registry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFakeScriptObject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_pytree.py\u001b[0m in \u001b[0;36mtree_any\u001b[0;34m(pred, tree, is_leaf)\u001b[0m\n\u001b[1;32m   1643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1645\u001b[0;31m def tree_any(\n\u001b[0m\u001b[1;32m   1646\u001b[0m     \u001b[0mpred\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[0mtree\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPyTree\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}